{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e9b91a",
   "metadata": {},
   "source": [
    "Previous Work in this feild:\n",
    "\n",
    "1. Machine Learning Approaches\n",
    "\n",
    "Classical ML Models:\n",
    "\n",
    "NaÃ¯ve Bayes, SVM, Neural Networks used with features from Twitter API (accuracy up to 99.9%).\n",
    "\n",
    "Count Vector / TF-IDF were widely used for feature extraction.\n",
    "\n",
    "Logistic Regression, Random Forest, Gradient Boosting, and AdaBoost were tested on datasets like Kaggle Fake News Challenge, LIAR, ISOT.\n",
    "\n",
    "NaÃ¯ve Bayes showed strong results (up to 99% accuracy).\n",
    "\n",
    "Feature Engineering:\n",
    "\n",
    "Document frequency features with propensity score matching (PSM) to reduce bias.\n",
    "\n",
    "N-grams (uni/bi/trigrams), TF, TF-IDF, embeddings.\n",
    "\n",
    "Combining textual features + metadata (e.g., site URL credibility).\n",
    "\n",
    "Some models included sentiment analysis and contextual features.\n",
    "\n",
    "Performance:\n",
    "\n",
    "Results varied, e.g. 68% (with PSM) to ~99% with ensemble/tree-based methods.\n",
    "\n",
    "Logistic regression, Random Forest, and NaÃ¯ve Bayes consistently strong across datasets.\n",
    "\n",
    "ðŸ”¹ 2. Deep Learning Approaches\n",
    "\n",
    "Word Embedding + Neural Networks:\n",
    "\n",
    "Word2Vec, GloVe, FastText used as embeddings.\n",
    "\n",
    "Capsule Neural Networks, CNNs, and Bi-LSTM achieved >90% accuracy.\n",
    "\n",
    "Language-Specific Work:\n",
    "\n",
    "Bengali â†’ Gaussian NaÃ¯ve Bayes on TF-IDF (87%).\n",
    "\n",
    "Arabic â†’ Sentiment features + POS tagging, achieving ~70â€“76%.\n",
    "\n",
    "Amharic â†’ FastText embeddings, Bi-LSTM (>99%).\n",
    "\n",
    "Slovak â†’ CNNs and LSTMs (92â€“93%).\n",
    "\n",
    "Malayalam â†’ XLM-RoBERTa gave 89â€“90%.\n",
    "\n",
    "Advanced Architectures:\n",
    "\n",
    "Graph-based neural networks (SemSeq4FD) used for sentence-level semantics (92.6%).\n",
    "\n",
    "Transformer-based models (TruthSeeker, hybrid Bi-LSTM + Transformer) achieved >99.9%.\n",
    "\n",
    "Multi-modal models (MM-FND) combining TF-IDF + Word2Vec + NER features achieved 95â€“97% across ISOT, LIAR, COVID datasets.\n",
    "\n",
    "ðŸ”¹ 3. Hybrid (ML + DL) Approaches\n",
    "\n",
    "Stacking Models: Combined ML and DL approaches with TF-IDF + embeddings, achieving 96â€“99% accuracy.\n",
    "\n",
    "BERT Models: Widely tested, often outperforming traditional ML/DL (up to 99.23%).\n",
    "\n",
    "Comparisons: CNNs, LSTMs, and BERT compared; BERT models gave the best accuracy (~98â€“99%).\n",
    "\n",
    "Ensemble + Hybrid Approaches: TF-IDF features combined with CNN architectures reached ~99% accuracy.\n",
    "\n",
    "ðŸ”¹ 4. Optimization Techniques\n",
    "\n",
    "Metaheuristics: Grey Wolf Optimization (GWO), Particle Swarm Optimization (PSO), Genetic Algorithms (GA), and Negative Swarm Optimization were applied for feature selection + classification.\n",
    "\n",
    "Results: GWO outperformed others, reaching 96.5% accuracy on LIAR dataset.\n",
    "\n",
    "Feature Selection: Reducing redundancy improved performance in some studies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b29115",
   "metadata": {},
   "source": [
    "# ðŸ“Š Fake News Detection â€“ Comparison of Methods, Datasets & Accuracy  \n",
    "\n",
    "| **Approach**              | **Models/Techniques**                                     | **Datasets**                        | **Accuracy / Performance**            |\n",
    "|----------------------------|-----------------------------------------------------------|--------------------------------------|---------------------------------------|\n",
    "| **Traditional ML**         | NaÃ¯ve Bayes, SVM, Logistic Regression, Random Forest, Gradient Boosting, AdaBoost | Twitter API, Kaggle Fake News, LIAR, ISOT | 68% â€“ 99% (NaÃ¯ve Bayes & RF often >95%) |\n",
    "| **Feature Engineering**    | TF, TF-IDF, N-grams, embeddings + metadata (URL, sentiment, POS) | Kaggle, BuzzFeed, LIAR               | ~70â€“96%                               |\n",
    "| **ML + PSM**               | Logistic Regression + Propensity Score Matching (PSM) with doc frequency | Twitter dataset                      | ~68%                                  |\n",
    "| **Deep Learning (NN)**     | CNN, Bi-LSTM, Capsule NN, Word2Vec, GloVe, FastText       | ISOT, LIAR, custom (Amharic, Slovak, Arabic, Bengali) | 87% (Bengali) â†’ 99% (Amharic) |\n",
    "| **Language-Specific DL**   | Bi-LSTM, XLM-RoBERTa, CNN                                | Malayalam, Arabic, Amharic, Slovak   | 70â€“93% (Arabic/Slovak) â†’ 99% (Amharic) |\n",
    "| **Graph Neural Networks**  | SemSeq4FD (semantic seq graph-based NN)                  | Fake news datasets                   | ~92.6%                                |\n",
    "| **Transformers**           | BERT, TruthSeeker, Hybrid (Bi-LSTM + Transformer)        | ISOT, LIAR, COVID datasets           | 97% â€“ 99.9%                           |\n",
    "| **Hybrid ML + DL**         | TF-IDF + Embeddings + CNN/Stacked models                 | ISOT, LIAR                           | 96% â€“ 99%                             |\n",
    "| **Multimodal Models**      | MM-FND (text + NER + metadata + embeddings)              | ISOT, LIAR, COVID                    | 95% â€“ 97%                             |\n",
    "| **Optimization Techniques**| Grey Wolf Optimizer (GWO), PSO, GA                       | LIAR                                 | GWO: 96.5% (best), others slightly lower |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eefe02",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
