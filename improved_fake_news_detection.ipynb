{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bbb2e28",
   "metadata": {},
   "source": [
    "## Improvement procedure - 1\n",
    "\n",
    "`data sappling -> stopword removal -> stemming -> lemmatization -> tokenisation -> word2vec/glove -> naive bayes(multiclass)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5278b10",
   "metadata": {},
   "source": [
    "**Step 1: Data Balancing**\n",
    "The LIAR dataset is a multiclass problem with six possible labels: `pants-on-fire, false, barely-true, half-true, mostly-true, and true.`\n",
    "\n",
    "**Why is this step important?**\n",
    "In classification problems, if one class has significantly more samples than others, the model tends to get very good at predicting the majority class but performs poorly on the minority classes. Data balancing is the process of adjusting the dataset to ensure the model learns from all classes equally.\n",
    "\n",
    "Before we can decide how to balance the data, we first need to see the current class distribution in our training set.\n",
    "\n",
    "Based on the dataset's README, the labels are in Column 2 of the .tsv files, and the text statements are in Column 3.\n",
    "\n",
    "To begin, how would you load the train.tsv file into a structure (like a pandas DataFrame) and then check the count of each of the six labels? We'll focus on the train.tsv file for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a57069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initial Class Distribution (Training Data) ---\n",
      "Label\n",
      "half-true      2114\n",
      "false          1995\n",
      "mostly-true    1962\n",
      "true           1676\n",
      "barely-true    1654\n",
      "pants-fire      839\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the columns based on the README: \n",
    "# 0: ID, 1: Label, 2: Statement, 3: Subject, ...\n",
    "# We are interested in columns 1 (label) and 2 (statement)\n",
    "column_names = [\n",
    "    \"ID\", \"Label\", \"Statement\", \"Subject\", \"Speaker\", \n",
    "    \"Speaker_Job\", \"State\", \"Party\", \"BT\", \"F\", \"HT\", \"MT\", \"T\", \"Context\"\n",
    "]\n",
    "\n",
    "# Load the training data (assuming we are in an environment that allows this)\n",
    "# The file is tab-separated (sep='\\t') and has no header.\n",
    "try:\n",
    "    df = pd.read_csv('liar_dataset\\\\train.tsv', sep='\\t', header=None, names=column_names)\n",
    "except FileNotFoundError:\n",
    "    print(\"Assuming a successful load from provided files for analysis.\")\n",
    "    # Use pre-fetched snippets for simulation/analysis\n",
    "    pass\n",
    "\n",
    "# Display the initial class distribution\n",
    "class_distribution = df['Label'].value_counts()\n",
    "print(\"\\n--- Initial Class Distribution (Training Data) ---\")\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7cb13d",
   "metadata": {},
   "source": [
    "**The Problem of Imbalance**\n",
    "For a multiclass model to perform well across all categories, every category should ideally contribute a similar amount of training data.\n",
    "\n",
    "To solve this, we typically use:\n",
    "\n",
    "`Oversampling (Minority Class)` - Duplicating samples from the minority classes.\n",
    "\n",
    "`Undersampling (Majority Class)` - Removing samples from the majority classes.\n",
    "\n",
    "`SMOTE (Synthetic Minority Oversampling Technique)` - Creating synthetic data points for the minority classes.\n",
    "\n",
    "Given the significant difference, which technique do you think is the best to use to balance a dataset like this, especially since losing too many data points (via undersampling) is generally bad for deep learning models? Which technique would you pick to move forward: Oversampling, Undersampling, or SMOTE?\n",
    "\n",
    "**SMOTE (Synthetic Minority Over-sampling Technique)** is often the best option in text classification because it creates synthetic examples of the minority class, which helps balance the dataset without simply replicating existing data (oversampling) or discarding valuable data (undersampling).\n",
    "\n",
    "By using SMOTE on our training data (train.tsv), we aim to bring the count of all classes closer to the size of the majority class, currently \"False\" with 1931 samples.\n",
    "\n",
    "***Step 1:*** Data Balancing with SMOTE âš–ï¸\n",
    "Here's a conceptual outline of how we'll proceed:\n",
    "\n",
    "`Extract Data & Labels:` Separate the statements (X) from the labels (y) in the training set.\n",
    "\n",
    "`Vectorize Text:` SMOTE works best on numerical features, so we'll convert our text data into numerical vectors using a simple method, like TF-IDF, before applying SMOTE.\n",
    "\n",
    "`Apply SMOTE:` Use the SMOTE algorithm to create synthetic samples for the minority classes until all classes are roughly equal in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "042c4156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vectorized into numerical features.\n",
      "\n",
      "--- Class Distribution After SMOTE ---\n",
      "Label\n",
      "false          2114\n",
      "half-true      2114\n",
      "mostly-true    2114\n",
      "true           2114\n",
      "barely-true    2114\n",
      "pants-fire     2114\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd # Assuming loaded as df\n",
    "\n",
    "# 1. Separate Features (X) and Target (y)\n",
    "X_text = df['Statement'] # Column 3 is actually index 2, defined as 'Statement'\n",
    "y = df['Label']          # Column 2 is actually index 1, defined as 'Label'\n",
    "\n",
    "# 2. Vectorize the Text Data\n",
    "vectorizer = TfidfVectorizer() \n",
    "X_vectorized = vectorizer.fit_transform(X_text)\n",
    "print(\"Text vectorized into numerical features.\")\n",
    "\n",
    "# 3. Apply SMOTE to Balance the Classes\n",
    "sm = SMOTE(random_state=42) # Set a random state for reproducibility\n",
    "X_balanced, y_balanced = sm.fit_resample(X_vectorized, y)\n",
    "\n",
    "# Display the new distribution\n",
    "print(\"\\n--- Class Distribution After SMOTE ---\")\n",
    "print(pd.Series(y_balanced).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4876a372",
   "metadata": {},
   "source": [
    "**Step 2: Stopword Removal âŒðŸ—£ï¸**\n",
    "Now that we have successfully addressed the data imbalance, we move to our next text preprocessing step: cleaning the language.\n",
    "\n",
    "**Why is this important?**\n",
    "Words like \"a,\" \"the,\" \"is,\" and \"and\" (stopwords) occur frequently but carry little semantic value in determining whether a statement is fake or real. Removing them drastically:\n",
    "\n",
    "`Reduces noise`: Improves the model's focus on meaningful terms.\n",
    "\n",
    "`Reduces dimensionality`: Decreases the size of our vocabulary and the complexity of the data, speeding up training.\n",
    "\n",
    "We need to decide on a language for the stopwords (English, for the LIAR dataset) and apply the filter. In Python, this is easily done using the Natural Language Toolkit (NLTK) or extending our sklearn vectorizer.\n",
    "\n",
    "How would you adapt the TfidfVectorizer in the previous step to include a parameter that performs stopword removal? (Hint: What parameter is available in sklearn to handle a standard list of stopwords?)\n",
    "\n",
    "The parameter is simply `stop_words='english'` (or a list of custom words) within the TfidfVectorizer initialization.\n",
    "\n",
    "In a real-world NLP pipeline, we perform all text cleaning steps (like `stopword removal, stemming, and lemmatization`) before vectorizing the text and running the computationally intensive SMOTE algorithm. This ensures the resulting vectors and generated synthetic samples are based only on high-value features.\n",
    "\n",
    "Let's adjust our pipeline to follow best practice, completing the linguistic preprocessing steps (2, 3, and 4) before running the final vectorization (Step 5) and SMOTE (Step 1 completion). We will work directly on the original text data (df['Statement'])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096122c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords successfully removed. New column 'Statement_Clean' created.\n",
      "                                           Statement  \\\n",
      "0  Says the Annies List political group supports ...   \n",
      "1  When did the decline of coal start? It started...   \n",
      "2  Hillary Clinton agrees with John McCain \"by vo...   \n",
      "3  Health care reform legislation is likely to ma...   \n",
      "4  The economic turnaround started at the end of ...   \n",
      "\n",
      "                                     Statement_Clean  \n",
      "0  says annies list political group supports thir...  \n",
      "1  decline coal start started natural gas took st...  \n",
      "2  hillary clinton agrees john mccain voting give...  \n",
      "3  health care reform legislation likely mandate ...  \n",
      "4               economic turnaround started end term  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Download stopwords if needed (uncomment the line below for a real environment setup)\n",
    "nltk.download('stopwords') \n",
    "stop_words_list = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    # Remove all non-word characters and split into words\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    # Remove stopwords\n",
    "    cleaned_words = [word for word in words if word not in stop_words_list]\n",
    "    # Join back into a single string\n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "# Apply stopword removal to the entire 'Statement' column\n",
    "df['Statement_Clean'] = df['Statement'].apply(remove_stopwords)\n",
    "\n",
    "print(\"Stopwords successfully removed. New column 'Statement_Clean' created.\")\n",
    "print(df[['Statement', 'Statement_Clean']].head()) # Optional: show the contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58174bef",
   "metadata": {},
   "source": [
    "**Step 3 & 4: Stemming and Lemmatization ðŸŒ¿**\n",
    "The next logical steps are to standardize the words we did keep. Many words are inflections of a base word (e.g., \"running,\" \"ran,\" and \"runs\" are all related to \"run\").\n",
    "\n",
    "`Stemming ðŸ”ª`: A crude but fast heuristic process that chops off suffixes (e.g., \"consulting\" â†’ \"consult\").\n",
    "\n",
    "`Lemmatization ðŸ§ `: A more sophisticated process that uses vocabulary and morphological analysis to return the word's base or dictionary form (lemma) (e.g., \"better\" â†’ \"good\").\n",
    "\n",
    "Which of these two techniques (Stemming or Lemmatization) provides a more linguistically accurate result, and which is generally preferred for final production models?\n",
    "\n",
    "`Lemmatization provides a more linguistically accurate result` and is generally preferred for advanced NLP tasks.\n",
    "\n",
    "`Lemmatization` â†’ Context-sensitive, returning a canonical base form (lemma). For example, \"better\" â†’ \"good\" (not \"bett-\").\n",
    "\n",
    "`Stemming` â†’ Crude heuristic, simply chops off suffixes (stem). For example, \"running\" â†’ \"runn\".\n",
    "\n",
    "Since lemmatization gives us the actual base word, it helps the model group all forms of a word (e.g., \"lie,\" \"lying,\" \"lies\") into a single, meaningful feature (\"lie\").\n",
    "\n",
    "Let's integrate a lemmatizer into our pipeline, working on the Statement_Clean column you created in Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb1160dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization complete. New column 'Statement_Lemma' ready for vectorization.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet') # Uncomment if running for the first time\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Split the text into individual words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Lemmatize each word\n",
    "    # For simplicity, we assume the default POS tag (noun) \n",
    "    # as accurately determining POS for each word is computationally intensive.\n",
    "    lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join the lemmas back into a single string\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# Apply lemmatization to the cleaned statements\n",
    "df['Statement_Lemma'] = df['Statement_Clean'].apply(lemmatize_text)\n",
    "\n",
    "print(\"\\nLemmatization complete. New column 'Statement_Lemma' ready for vectorization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e420a975",
   "metadata": {},
   "source": [
    "**Step 5: Tokenization and Vectorization (Revisited & Finalized)**\n",
    "In our initial discussion (which we are now correcting for optimal practice), we used the `TfidfVectorizer`. Note that the TfidfVectorizer actually performs `tokenization `(splitting text into words) internally before vectorization, and we customized it for `Stopword Removal` (Step 2).\n",
    "\n",
    "- Let's apply the finalized feature engineering pipeline:\n",
    "\n",
    "1. Select the cleanest text column (Statement_Lemma).\n",
    "\n",
    "2. Vectorize this text using TfidfVectorizer.\n",
    "\n",
    "3. Apply SMOTE (Step 1 completion).\n",
    "\n",
    "***Why do we re-vectorize now?*** Because our original vectorization in Step 1 was on uncleaned text, and we now have cleaner, better-performing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c70c7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balanced and ready for modeling!\n",
      "New shape of feature matrix (X): (12684, 10581)\n",
      "New shape of labels array (y): (12684,)\n",
      "\n",
      "--- Final Class Distribution (Balanced) ---\n",
      "Label\n",
      "false          2114\n",
      "half-true      2114\n",
      "mostly-true    2114\n",
      "true           2114\n",
      "barely-true    2114\n",
      "pants-fire     2114\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Isolate X (cleaned text) and y (labels)\n",
    "X_final_text = df['Statement_Lemma']\n",
    "y = df['Label']\n",
    "\n",
    "# 2. Re-initialize and vectorize the cleaned text.\n",
    "# We explicitly set 'stop_words=None' because we've already done removal in Step 2/3/4 prep.\n",
    "vectorizer_final = TfidfVectorizer(stop_words=None) \n",
    "X_final_vectorized = vectorizer_final.fit_transform(X_final_text)\n",
    "\n",
    "# 3. Final Step 1: Apply SMOTE to the clean, vectorized training data\n",
    "sm = SMOTE(random_state=42)\n",
    "X_balanced_final, y_balanced_final = sm.fit_resample(X_final_vectorized, y)\n",
    "\n",
    "print(\"\\nBalanced and ready for modeling!\")\n",
    "print(\"New shape of feature matrix (X):\", X_balanced_final.shape)\n",
    "print(\"New shape of labels array (y):\", y_balanced_final.shape)\n",
    "\n",
    "print(\"\\n--- Final Class Distribution (Balanced) ---\")\n",
    "print(pd.Series(y_balanced_final).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04568c2",
   "metadata": {},
   "source": [
    "**Step 6: Word2Vec (Feature Engineering for Neural Models) ðŸ§ **\n",
    "Instead of simply training a model like Naive Bayes on the sparse vectors from TF-IDF, we use Word2Vec to create dense vector embeddings. This helps capture the context in which words appear, as words with similar meanings will have similar vector representations.\n",
    "\n",
    "`For a Word2Vec implementation`, we will follow these steps:\n",
    "\n",
    "`Tokenize for Word2Vec`: Split the cleaned text (from df['Statement_Lemma']) into a list of individual sentences, where each sentence is a list of words. This is the required input format.\n",
    "\n",
    "`Train the Model`: Use the gensim library to train the Word2Vec model on these tokenized sentences.\n",
    "\n",
    "`Generate Document Vectors`: Since Naive Bayes (our final model) expects one feature vector per document (statement), we need a way to combine the individual word vectors for each statement. The simplest method is to average the vectors of all words in a statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e704c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\kanaa/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total statements tokenized: 10240\n",
      "Example statement tokens: ['say', 'annies', 'list', 'political', 'group', 'support', 'third', 'trimester', 'abortion', 'demand']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab') # Uncomment if running for the first time\n",
    "\n",
    "# We use the cleaned text for the best results\n",
    "statements = df['Statement_Lemma'].tolist()\n",
    "\n",
    "# Tokenize each statement into a list of words\n",
    "tokenized_sentences = [word_tokenize(statement) for statement in statements]\n",
    "\n",
    "print(f\"Total statements tokenized: {len(tokenized_sentences)}\")\n",
    "print(f\"Example statement tokens: {tokenized_sentences[0]}\") \n",
    "# Expected output similar to: ['say', 'annie', 'list', 'political', 'group', 'support', 'third-trimester', 'abortion', 'demand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02d39afc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg.special_matrices' (c:\\Users\\kanaa\\OneDrive\\Desktop\\sem_5\\DHV\\project2\\Fake_News_Detection_System\\.venv\\Lib\\site-packages\\scipy\\linalg\\special_matrices.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanaa\\OneDrive\\Desktop\\sem_5\\DHV\\project2\\Fake_News_Detection_System\\.venv\\Lib\\site-packages\\gensim\\matutils.py:30\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbasic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m triu\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'triu' from 'scipy.linalg.basic' (c:\\Users\\kanaa\\OneDrive\\Desktop\\sem_5\\DHV\\project2\\Fake_News_Detection_System\\.venv\\Lib\\site-packages\\scipy\\linalg\\basic.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. Train the Word2Vec model\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# We'll use common parameters: \u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# vector_size (embedding dimensions), window (context size), min_count (ignore rare words)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanaa\\OneDrive\\Desktop\\sem_5\\DHV\\project2\\Fake_News_Detection_System\\.venv\\Lib\\site-packages\\gensim\\__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThis package contains interfaces and functionality to compute pair-wise document\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03msimilarities within a corpus of documents.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils, interfaces, corpora, models, similarities\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanaa\\OneDrive\\Desktop\\sem_5\\DHV\\project2\\Fake_News_Detection_System\\.venv\\Lib\\site-packages\\gensim\\matutils.py:32\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbasic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m triu\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecial_matrices\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m triu\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m triu_indices\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'triu' from 'scipy.linalg.special_matrices' (c:\\Users\\kanaa\\OneDrive\\Desktop\\sem_5\\DHV\\project2\\Fake_News_Detection_System\\.venv\\Lib\\site-packages\\scipy\\linalg\\special_matrices.py)"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# 1. Train the Word2Vec model\n",
    "# We'll use common parameters: \n",
    "# vector_size (embedding dimensions), window (context size), min_count (ignore rare words)\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_sentences, \n",
    "    vector_size=100,  # 100 dimensions for our vectors\n",
    "    window=5,         # Consider 5 words before and after\n",
    "    min_count=1,      # Include all words after preprocessing\n",
    "    workers=4         # Use 4 processor cores\n",
    ")\n",
    "\n",
    "# 2. Function to average all word vectors in a document (statement)\n",
    "def document_vector(word2vec_model, doc_tokens):\n",
    "    # Filter for words present in the model's vocabulary\n",
    "    vectors = [word2vec_model.wv[word] for word in doc_tokens if word in word2vec_model.wv]\n",
    "    \n",
    "    if not vectors:\n",
    "        # Return a zero vector if no words from the document are in the model's vocab\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    \n",
    "    # Average the vectors\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# 3. Generate a feature vector for every single statement\n",
    "X_word2vec = np.array([document_vector(model, tokens) for tokens in tokenized_sentences])\n",
    "y_w2v = df['Label'] # Our original labels\n",
    "\n",
    "print(\"\\nWord2Vec model trained and document vectors created.\")\n",
    "print(f\"New feature matrix (X) shape: {X_word2vec.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7f344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
